---
title: "Predicting Loneliness from Social, Technology, and Demographic Factors"
author: "Jean Singer"
date: "9/4/2022"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
bibliography: references.bib
---

# 1. Introduction

## 1.1 Background

Loneliness in the US has reached what has been described as epidemic proportions. In a 2020 study of more than 10,400 adults by the health insurer Cigna, roughly three in five Americans (61%) were classified as lonely[@coombs2020]. Compared to 2018, when the survey was first introduced, loneliness rates rose by roughly 7 percentage points. Studies using different methodologies show various rates of loneliness but confirm that a sizeable proportion of the country suffers from this condition. The Covid-19 pandemic may also be playing a role in the rise of loneliness. In October 2020, Harvard University researchers found that 36% of Americans felt "serious loneliness" while two months prior, the proportion was 25%[@weissbourd].

Social connection is a fundamental human need, akin to our needs for food and warmth [@lieberman2013]. When our social needs are insufficiently met, we feel loneliness, which has been defined as "a distressing feeling that accompanies the perception that one's social needs are not being met by the quantity or especially the quality of one's social relationships"[@hawkley2010]. Loneliness is a signal that we need to improve social connection---and our biology responds if we don't. Chronic loneliness has been associated with increased risk for a host of physical and mental health issues, including a 26% increase in the risk for early mortality, a rate comparable to those for obesity, inactivity, and smoking 15 cigarettes a day[@holt-lunstad2010a; @holt-lunstad2015b]. Loneliness has also been associated with increased risks for coronary heart disease and stroke[@everson-rose2005], impaired immune function [@cohen1997], cognitive decline and dementia [@cacioppo2014; @kim2017]. Given the reported rates of loneliness and the prospect that it may be further on the rise, we need to address loneliness as a serious public health concern.

Loneliness is a *measurable* *feeling* that may be influenced by a number of *objective factors*, such as the size or diversity of our social networks, the amount of time we spend alone, or the ways in which we use technology. If we can better understand the drivers behind loneliness and quantify their impact, we can more effectively target the factors that matter most. Machine learning models that predict loneliness from social, technology and demographic factors can help us to identify relevant factors to target.

## 1.2 Project Objectives

The objective of this project was to build a machine learning model to predict loneliness from a set of social, technology and demographic factors. The data I used was collected by AARP for a study of loneliness among people age 45 and above. AARP performed various analyses on their data, including a linear regression model predicting loneliness.

In the current project, I sought to:

-   Build a linear regression model with better predictive ability than the one built by AARP.

-   Further optimize predictive ability with three additional machine learning models: XGBoost, PCR and an ensemble.

-   Use machine learning models to identify the social, technology and demographic features most important to predicting loneliness.

## 1.3 Dataset and Variables

In 2018, AARP conducted a study of loneliness and how it relates to social connections, life experiences, health, and technology among adults age 45 and over. They contacted a nationally-representative sample of 6343 people for participation in a web-based survey, and obtained a response rate of 50.8%, or 3223 people. I removed from the dataset 20 people (0.6%) who did not complete the key outcome measure (UCLA Loneliness Index.) The final dataset consisted of 3203 observations.

The main outcome variable was loneliness, measured by the UCLA Loneliness Scale [@russell1996], a validated and widely-used index of 20 items measuring dimensions of loneliness on a 4-point scale from "never" to "always." The index is created by summing the responses. The possible range of responses is 20 (least lonely) to 80 (most lonely), and AARP classifies a score of 44 or higher as "lonely."

The AARP dataset I started with contained 67 predictor variables. The variables included measures of social connection, technology use, life events such as moving or death of someone close, activities such as volunteering and belonging to groups, attitudes regarding the internet, health, and demographics. A list of the variables is provided in [Appendix A: AARP Variables].

The AARP variables did not include a measure of complex social integration (CSI), which is an indicator of the extent to which an individual participates in a wide range of social activities and relationships[@brissette2000]. CSI has been frequently associated with increased risk of mortality [@holt-lunstad2010b; @berkman1979; @berkman2004] and morbidity [@cohen1997a; @coyle2012], and is a commonly-used measure in studies of social connection and isolation. I created an index of CSI using the variable DiversitySupportive (a measure of the diversity of people who have been supportive of you in the last year, across friends, spouse, children, parents, other relatives, neighbors, co-workers, and others) and adding points for being married or living with a partner, being employed, volunteering, membership in groups. This approach is similar to those used by researchers to create other established indexes such as the Berkman-Syme Social Network Index [@berkman1979a] and the Cohen Social Network Index [@cohen1997c].

Here I load the dataset and add the CSI variable. (Code not shown in pdf version.)

```{r load dataset and add CSI, message=FALSE, warning=FALSE, include=FALSE}

#set working directory
setwd("C:/Users/Jean/Desktop/capstone CYO project/R working files")

library(tidyverse)

#read in the data file
library(readxl)
df <- read_excel("AARPdata_for_capstone.xlsx")

#-------------------------------------------------------------------
#Create the variable for Complex Social Integration (CSI)
#-------------------------------------------------------------------
#CSI starts with the value for DiversitySupportive and adds point for marital satisfaction,
#employment status, volunteer work, and group membership.

#marital (), employed (),
#volunteer (yes (1) gets a point), and groups (
#-------------------------------------------------------------

#Create variable with points to be added for marital: marital satisfaction
#greater than 3 gets a point.
df <- df %>% mutate(CSImarital = ifelse(Q22_marital_satn>3, 1, 0))

#Create variable with points to be added for employed: 1 or 2 gets a point
df <- df %>% mutate(CSIemployed = ifelse(employ==1 |employ==2, 1, 0))

#Create variable with points to be added for volunteer: 1 gets a point
df <- df %>% mutate(CSIvolunteer = ifelse(Q48_volunteer==1, 1, 0))

#Create variable with points to be added for groups: 2 or more (meaning 1-3 groups)gets a point
df <- df %>% mutate(CSIgroups = ifelse(Q50_groups > 1, 1, 0))

#Creat the index by adding the variables above to DiversitySupportive
df$DiversitySupportive <- as.numeric(df$DiversitySupportive)
df <- df %>% mutate(CSI = rowSums(across(c(DiversitySupportive,CSImarital, CSIemployed,CSIvolunteer, CSIgroups)),na.rm = TRUE))

#convert 0s to NAs
df$CSI[df$CSI==0] <- NA

#remove the point calculations so they don't show up as variables
df <- df %>% select (-c(CSImarital, CSIemployed, CSIvolunteer, CSIgroups))

```

## 1.4 Key Steps Performed

I started with a process of exploring the data and selecting the variables that were most likely to be predictive in my models. Using those variables, I built three machine learning models: linear regression, XGBoost (Extreme Gradient Boosting), and principal components regression (PCR.) I evaluated my linear regression model against AARP's linear regression model using R^2^ as my performance measure. I also evaluated each of my models against each other using RMSE. (AARP did not report RMSE for their model.) I combined the three models into an ensemble and compared RMSE to the individual models.

I used the linear regression and XGBoost models to generate measures of variable importance. (PCR and the ensemble do not provide variable importance metrics.) I then examined the top five most important variables and compared them between the two models to see were they found the same variables to be important.

# 2. Methods and Analysis

## 2.1 Exploratory Analysis and Variable Selection

I examined the structure of the data. The dataset contains 3203 rows and 70 variables: 68 predictor variables, one outcome variable and the respondent number. All variables are shown as numeric, although some should be factor type. I will convert them later on.

```{r structure}

str(df)
```

### 2.1.1 Exploratory Analysis of Outcome Variable

For the main outcome variable---loneliness as measured by the UCLA Loneliness Index---I generated a histogram, examined the distribution, and examined incidence of loneliness by age.

Here is the histogram:

```{r loneliness histogram}

#create a histogram of the loneliness variable
hist(df$UCLA_index, main = "Distribution of Loneliness Scores",
     xlab = "UCLA Loneliness Index", ylab = "Count", col = "sky blue", border = "black")

```

The loneliness histogram is skewed right. A relatively large number of respondents are at the lowest end of the scale compared to the number at the highest end of the scale.

This deviation from the normal distribution can also be seen in the Q-Q plot. The distribution follows the Q-Q line fairly well except for the lowest values.

```{r Q-Q plot of loneliness}

#create a Q-Q plot of loneliness
qqnorm(df$UCLA_index)
qqline(df$UCLA_index)

```

The chart below shows the mean, standard deviation, median and mode values for the loneliness variable. The mean is higher than the median, as would be expected in a right-skewed distribution.

```{r stats for loneliness, message=FALSE, warning=FALSE}

#compute the mean, sd, median and mode for loneliness
mean_loneliness <- mean(df$UCLA_index)
sd_loneliness <- sd(df$UCLA_index)
median_loneliess <- median(df$UCLA_index)
library(DescTools)
mode_loneliness <- Mode(df$UCLA_index)

#place the stats in a table
descriptive_stats <- tibble(Statistic = c("Mean", "SD", "Median", "Mode"),
                            Value = c(mean_loneliness, sd_loneliness, 
                                      median_loneliess,mode_loneliness))
descriptive_stats %>% knitr::kable()

```

Using the AARP cutoff of 44 and above to define "lonely", I found that a little over a third (35.6%) of the sample would be considered lonely.

```{r percentage lonely}

#compute the percentage of people who would be considered lonely (UCLA index >=44)
mean(df$UCLA_index >=44)
```

A plot of average loneliness by age group shows that loneliness declines with age. This finding is consistent with other studies indicating that loneliness is highest among young people (around age 16 -- 24) and declines after middle age, although some studies show loneliness increasing at age 75 and above [@hawkley2019; @communit].

```{r plot of loneliness by age, message=FALSE, warning=FALSE}

#Percentage of people classified as "lonely" by age
df %>% group_by(age_group) %>% 
  summarise(Percentage_lonely = mean(UCLA_index >= 44)) %>%
  ggplot(aes(age_group, Percentage_lonely)) + geom_bar(stat = "identity", 
                                                       col = "black", fill = "sky blue") +
  xlab("Age Group") + ylab("Percentage Lonely (UCLA Index >=44)") +
  labs(title = "Loneliness by Age") + theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
  scale_y_continuous(labels = scales::percent_format(accuracy=1)) +
  scale_x_continuous(breaks = 5:8, labels = c("45 - 54", "55 - 64", "65 - 74", "75 and over"))
```

### 2.1.2 Exploratory Analysis of Predictors and Variable Selection

To identify the predictor variables that would be most useful in predicting loneliness, I conducted three types of analyses: correlation with loneliness, ANOVA, and t-tests.

*Correlation with loneliness*. I treated ordinal variables with 5 or more response options and any index variables as "pseudo-continuous", and generated Pearson's correlation coefficients for them. I selected all variables with correlations of greater than .20 or less than -.20, and p\<.05 for use in my machine learning models. Out of 55 variables, only 13 met this fairly low hurdle, indicating that individually, the variables were not very strong predictors. All p-values, however, were significant (p\<.001). The variables that met these criteria are shown in the table below.

Below is the code for running the correlations and selecting the variables that meet my criteria.

```{r correlations with loneliness, message=FALSE, warning=FALSE}

#Create a correlation matrix for the variables of interest, which are
#those that are indexes or ordinal with 5 or more response options
variables <- df %>% select(-c(respondent, ethnic,
                          Q22_marital_satn, Q38_more_less_friends,
                          Q50_groups,
                         Q90_tradeoffs_family, Q91_tradeoffs_intimate_convo,
                        age_group, employ, gender,
                        Q39_caregiver, Q48_volunteer, Q8_disability))

#Generate the correlation matrix and p values
library(Hmisc)
cor_variables <- rcorr(as.matrix(variables))
cor_variables_r <- round(cor_variables[["r"]],2) 
cor_variables_p <- round(cor_variables[["P"]],3) 

#In the r-value matrix, select the variables with correlations >.20 or <-0.2
new_variables <- as.data.frame(cor_variables_r) %>% 
  filter(UCLA_index>0.2 | UCLA_index<(-.2)) %>% 
  select(UCLA_index)

#create a column with the names of all the variables
new_variables$variable_names <- row.names(new_variables)

#remove the first row that contains UCLA_index as a variable
new_variables <- new_variables[-1, ]

#Now take a look at the p-values
#In the p-value matrix, add variable names as a column 
new_variables_p <- as.data.frame(cor_variables_p)
new_variables_p$variable_names <- row.names(new_variables_p)

#select the relevant variables from the p-value matrix
relevant_variables <- c("NeighborIndex", "DiversityDiscussImportant",
                        "DiversitySupportive", "Q30_supportive_num",
                        "income", "Q2_health_overall", 
                        "Q27_41_friend_inperson",
                        "Q27_43_friend_phone",
                        "Q28_discuss_important_matters_num",
                        "Q77_hrs_alone", "Q89_5_internet_sentiment",
                        "Q89_7_internet_sentiment", "CSI")

new_variables_p <- new_variables_p %>% filter(variable_names %in% relevant_variables) %>%
  select(UCLA_index, variable_names)

#add three decimal places to the p-value
new_variables_p[,'UCLA_index']=format(round(new_variables_p[,'UCLA_index'],3),nsmall=3)

#create a table with the variables, correlations and p-values for all variables
# with r>0.20
correlation_table <- tibble(Variable_name = new_variables$variable_names,
                            Pearsons_r = new_variables$UCLA_index,
                            p_value = new_variables_p$UCLA_index)
#sort the table
correlation_table <- correlation_table %>% arrange(desc(Pearsons_r))

#format the table
correlation_table %>% knitr::kable()

```

We can see from the table above that the only predictor positively associated with loneliness is number of hours spent alone (Q77_hrs_alone.) The positive relationship implies that the more hours we spend physically alone, the lonelier we feel. The strongest predictor overall (although still not a "strong" correlation) is complex social integration (CSI), which is negatively associated with loneliness. The negative relationship implies that the more we participate in a wide range of social activities and relationships, the less lonely we are.

I generated scatterplots of the 13 variables but because the correlations were low, the scatterplots were not informative. (Plots not shown.)

*ANOVA*. For variables that were nominal or that were ordinal with fewer than 5 responses, I ran ANOVAs to determine whether there was a statistically significant difference (p\<.05) in mean loneliness across each of the categorical groups. The 7 relevant variables were:

-   ethnic (ethnicity)

-   Q22_marital_satn (marital satisfaction)

-   Q38_more_less_friends (increase or decrease in number of friends over the past five years)

-   Q50_groups (number of groups you belong to)

-   Q90_tradeoffs_family (whether you spend more/less/same time with family as a result of the internet)

-   Q91_tradeoffs_intimate_convo (whether you spend more/less/same time in intimate conversations as a result of the internet)

-   age_group.

All seven variables showed significant differences in mean loneliness among their categorical groups (p\<.01 or better.) As a result, I included all of them in my models. Below is the code for running the seven ANOVAs.

```{r ANOVAs}

#Convert all the variables to factor
df$Q22_marital_satn <- as.factor(df$Q22_marital_satn)
df$age_group <- as.factor(df$age_group)
df$Q38_more_less_friends <- as.factor(df$Q38_more_less_friends)
df$Q90_tradeoffs_family <- as.factor(df$Q90_tradeoffs_family)
df$Q91_tradeoffs_intimate_convo <- as.factor(df$Q91_tradeoffs_intimate_convo)
df$Q50_groups <- as.factor(df$Q50_groups)
df$ethnic <- as.factor(df$ethnic)

#ANOVA of Q22_marital_satn vs loneliness
df_marital_satn <- df %>% 
  filter(Q22_marital_satn != "NA" & UCLA_index != "NA") 
ANOVA_marital_satn_loneliness <- aov(UCLA_index ~ Q22_marital_satn, 
                                     data = df_marital_satn)
summary(ANOVA_marital_satn_loneliness)

#ANOVA of ethnicity vs loneliness
df_ethnic <- df %>% 
  filter(ethnic != "NA" & UCLA_index != "NA") 
ANOVA_ethnic <- aov(UCLA_index ~ ethnic, 
                    data = df_ethnic)
summary(ANOVA_ethnic)

#ANOVA of age group vs loneliness 
df_agegroup <- df %>% 
  filter(age_group != "NA" & UCLA_index != "NA") 
ANOVA_agegroup_loneliness <- aov(UCLA_index ~ age_group, 
                                 data = df_agegroup)
summary(ANOVA_agegroup_loneliness)

#ANOVA of Q38_more_less_friends (changes in number of friends
#over the last 5 years) vs loneliness
df_friends <- df %>% 
  filter(Q38_more_less_friends != "NA" & UCLA_index != "NA") 
ANOVA_friends_loneliness <- aov(UCLA_index ~ age_group, 
                                data = df_friends)
summary(ANOVA_friends_loneliness)

#ANOVA of Q90_tradeoffs_family (as a result of technology, spending
#less, same or more time with family) vs loneliness
df_tradeoffsfam <- df %>% 
  filter(Q90_tradeoffs_family != "NA" & UCLA_index != "NA") 
ANOVA_tradeoffsfam_loneliness <- aov(UCLA_index ~ Q90_tradeoffs_family, 
                                     data = df_tradeoffsfam)
summary(ANOVA_tradeoffsfam_loneliness)

#ANOVA of Q91_tradeoffs_intimate_convo (as a result of technology, spending
#less, same or more time in intimate conversations) vs loneliness 
df_tradeoffsint <- df %>% 
  filter(Q91_tradeoffs_intimate_convo != "NA" & UCLA_index != "NA") 
ANOVA_tradeoffsint_loneliness <- aov(UCLA_index ~ Q91_tradeoffs_intimate_convo, 
                                     data = df_tradeoffsint)
summary(ANOVA_tradeoffsint_loneliness)

#ANOVA of Q50_groups (number of groups you belong to with 
#1=0, 2=1, 3=2, 4=3 or more) vs loneliness
df_groups <- df %>% 
  filter(Q50_groups != "NA" & UCLA_index != "NA") 
ANOVA_groups_loneliness <- aov(UCLA_index ~ Q50_groups, 
                               data = df_groups)
summary(ANOVA_groups_loneliness)
```

I generated boxplots for all seven variables. Below I show an example illustrating the association between loneliness and change in the number of friends over the last five years. The boxplot illustrates that loneliness on average is highest when the number of friends has declined, and lowest when the number of friends has increased. (The other six boxplots are not shown.)

```{r boxplot change in number of friends vs loneliness}

df %>% filter(Q38_more_less_friends != "NA") %>% 
  ggplot(aes(as.factor(Q38_more_less_friends), UCLA_index)) + geom_boxplot() +
  xlab("Change in Number of Friends over Past 5 Years") + ylab("Loneliness (UCLA Index)") +
  labs(title = "Loneliness by Change in Number of Friends")+
  scale_x_discrete(breaks = c("1", "2", "3"), 
                   labels = c("Fewer", "Same", "More"))
```

*T-TESTS.* For binary variables, I conducted t-tests and selected those variables whose levels show significant differences (p\<.05) in mean level of loneliness. Three variables showed significant differences and I included them in the models:

-   Q48_volunteer (whether you have volunteered in the last 12 months), p\<.001

-   Q8_disability (whether you have a disability or chronic disease that keeps you from participating fully in work, school, household, or other activities), p\<.001

-   Q39_caregiver (whether you are providing unpaid care or assistance to an adult who needs assistance due to aging, a disability, or a health-related issue), p\<.01

Two variables did not show significant differences between levels and were removed. These were gender (p = 0.17) and employed (p = .06).

Below is the code for running the five t-tests.

```{r t-tests}

#t-test on mean loneliness for Q48_volunteer (whether or not you have 
#in the last 12 months.)
df_volunteer <- df %>% 
  filter(Q48_volunteer != "NA" & UCLA_index != "NA") 
t.test(UCLA_index ~ Q48_volunteer, data = df_volunteer)

#t-test on mean loneliness for Q8_disabled (whether or not you are disabled)
df_disabled <- df %>% 
  filter(Q8_disability != "NA" & UCLA_index != "NA") 
t.test(UCLA_index ~ Q8_disability, data = df_disabled)

#t-test on mean loneliness for Q39_caregiver (whether or not you are
#providing unpaid care to an adult)
df_caregiver <- df %>% 
  filter(Q39_caregiver != "NA" & UCLA_index != "NA") 
t.test(UCLA_index ~ Q39_caregiver, data = df_caregiver)

#t-test on mean loneliness for gender
df_gender <- df %>% 
  filter(gender != "NA" & UCLA_index != "NA") 
t.test(UCLA_index ~ gender, data = df_gender)

#t-test on mean loneliness for whether or not you're employed
df_employed <- df %>% mutate(CSIemployed = ifelse(employ==1 |employ==2, 1, 0))%>%
  filter(CSIemployed != "NA" & UCLA_index != "NA") 
t.test(UCLA_index ~ CSIemployed, data = df_employed)
```

I generated boxplots on all five variables. Below I show examples for Q8_disability, where the means are significantly different, and gender, where they are not.

```{r boplots caregiver and gender}

#boxplot showing higher level of loneliness when you have a disability
df %>% filter(Q8_disability != "NA") %>% 
  ggplot(aes(as.factor(Q8_disability), UCLA_index)) + geom_boxplot() +
  xlab("Do you have a disability?") + ylab("Loneliness (UCLA Index)") +
  labs(title = "Comparing Loneliness by Disability Status") +
  scale_x_discrete(breaks = c("0", "1"), 
                   labels = c("No", "Yes"))

#boxplot showing no significant difference in loneliness between genders.
df %>% filter(gender != "NA") %>% 
  ggplot(aes(as.factor(gender), UCLA_index)) + geom_boxplot() +
  xlab("Gender") + ylab("Loneliness (UCLA Index)") +
  labs(title = "Comparing Loneliness by Gender") +
  scale_x_discrete(breaks = c("1", "2"),
                   labels = c("Male", "Female"))

```

My final dataset consisted of 23 predictors, each of which had either a Pearson's correlation coefficient with loneliness of greater than \|0.20\|, significance (p\<.01) in ANOVA or significance (p\<.05) in the t-test.

## 2.2 Modeling Approaches

I started with a linear regression model to see if the 23 variables I selected could out-perform the AARP linear regression model on the basis of R^2^. (It did.)

I then created an XGBoost model with the same variables. I chose XGBoost because I wanted to see if a tree-based model would be more accurate than a linear model, given that most of the linear relationships as measured by Pearson's correlation coefficient were weak, and some of my variables were categorical. I chose XGBoost rather than Random Forest because it is more efficient and accurate. Unlike Random Forest, which creates decision trees in parallel and averages them (for regression), XGBoost uses a *gradient boosting* technique in which decision trees are created sequentially. It increases the weights of variables that were predicted wrong by one tree prior to feeding it into the next tree. The result is often a more accurate model.

Although I reduced the number of predictors from 67 to 23, I still had a large number of variables. I therefore thought a dimension reduction approach would be helpful, and tried principal components regression (PCR.) PCR regresses the principal components of the predictors against the outcome variable rather than using predictors themselves. It can lower the number of parameters in the model. However, because the outcomes are described in terms of principal components rather than variables, it can be more difficult to interpret.

Finally, I created an ensemble as a way to combine multiple weaker models into a stronger model. I computed the ensemble prediction by taking the average of the predictions for the linear regression, XGBoost and PCR models.

I evaluated the relative performance of the linear regression, XGBoost, PCR and ensemble models on the basis of RMSE.

I identified the the variables that were most important to predicting loneliness in the linear regression and XGBoost models. In the regression model, I used the absolute value of the t-score as my indicator of variable importance. The rationale for using t-scores (rather than normalized coefficients) is that it gives us the variables that most certainly have non-zero effects and takes into account the uncertainty in the regression coefficients[@variable]. In the XGBoost model, I used the Gain metric generated by the xgb.importance function. Gain represents the fractional improvement in accuracy that a predictor brings to the branch that it is on. A higher percentage Gain indicates a more important predictor[@abu-rmileh2021].

I examined the top five and top ten most important variables in each model to see where there were commonalities.

# 3. Models and Results

Before building the models, I split the data 80/20 into training and test sets. All models were built on the training data and RMSE computed in the test set.

```{r training and test sets, message=FALSE, warning=FALSE}

#Create the relevant data frame with 23 predictor variables and the loneliness variable
variables <- df %>% select(UCLA_index, Q77_hrs_alone, Q27_43_friend_phone,
                           income, Q30_supportive_num, DiversityDiscussImportant,
                           Q89_7_internet_sentiment, Q89_5_internet_sentiment,
                           Q28_discuss_important_matters_num,Q2_health_overall,
                           Q27_41_friend_inperson, NeighborIndex, DiversitySupportive,
                           CSI, 
                           ethnic, Q22_marital_satn, Q38_more_less_friends, 
                           Q50_groups, Q90_tradeoffs_family,
                           Q91_tradeoffs_intimate_convo, age_group,
                           Q48_volunteer, Q8_disability,
                           Q39_caregiver)
                           
#convert binary and nominal variables to factor class 
variables$ethnic <- as.factor(variables$ethnic)
variables$Q22_marital_satn <- as.factor(variables$Q22_marital_satn)
variables$Q38_more_less_friends <- as.factor(variables$Q38_more_less_friends)
variables$Q50_groups <- as.factor(variables$Q50_groups)
variables$Q90_tradeoffs_family <- as.factor(variables$Q90_tradeoffs_family)
variables$Q91_tradeoffs_intimate_convo <- as.factor(variables$Q91_tradeoffs_intimate_convo)
variables$age_group <- as.factor(variables$age_group)
variables$Q48_volunteer <- as.factor(variables$Q48_volunteer)
variables$Q8_disability <- as.factor(variables$Q8_disability)
variables$Q39_caregiver <- as.factor(variables$Q39_caregiver)

#Make sure the outcome variable is in the last column
variables <- variables %>% select(-UCLA_index, UCLA_index)

#convert to a data frame
variables <- as.data.frame(variables)

#Split the data 80/20 into training and test sets.
library(caret)
set.seed(2, sample.kind="Rounding")
train_index <- createDataPartition(variables$UCLA_index, times = 1, p=.8, list = FALSE)
train <- variables[train_index, ]
test <- variables[-train_index, ]
```

## 3.1 Linear Regression Model

Below is the code for building the linear regression model, printing the summary with significance levels and R-squared, and computing RMSE in the test set.

```{r linear regression}

#Create the linear regression model using the training set
LM <- lm(UCLA_index ~ ., data = train)

#print out the summary of the model
LMsummary <- summary(LM)
LMsummary

#Create a function to compute RMSE
RMSE <- function(actual_ratings, predicted_ratings){
  sqrt(mean((actual_ratings - predicted_ratings)^2, na.rm = TRUE))
}

#generate predictions on the test set
predict_LM <- predict(LM, newdata = test)

#compute RMSE for the linear regression model
RMSE_LM <- RMSE(test$UCLA_index, predict_LM)
RMSE_LM

```

The linear regression model yields an adjusted R-squared of 41.53 and an RMSE in the test set of 7.950. The table below compares these results with the AARP linear regression. The AARP model, which contained over 50 variables yielded an R^2^ = 0.213. My linear regression model explained almost twice as much of the variance in the outcome variable.

```{r table linear regression}

#Store the linear regression results in a table and compare
#to the AARP performance
model_results <- tibble(Model = c("AARP", "Linear Regression"), 
                        Adjusted_R2 = c(.213, LMsummary$adj.r.squared),
                        RMSE = c(NA, RMSE_LM))
model_results %>% knitr::kable()
```

## 3.2 XGBoost Model

Below is the code for fitting and tuning the XGBoost model.

```{r XGBoost, message=FALSE, warning=FALSE}

#load the xgboost package
library(xgboost)

#Create separate objects for the predictor and outcome variables in the training set
train_x <- data.matrix(train[ ,-ncol(train)])
train_y <- train[ , ncol(train)]

#Create separate objects for the predictor and response variables in the test set
test_x <- data.matrix(test[ ,-ncol(test)])
test_y <- test[ , ncol(test)]

#Define the final training and testing sets. 
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

#Fit and tune the model. First define the watchlist
watchlist = list(train=xgb_train, test=xgb_test)

#Fit the XGBoost model and display the training and testing data at each round
set.seed(2, sample.kind="Rounding")
model_XG = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 70)

#Find the lowest RMSE and insert in the code for nrounds. 
set.seed(2, sample.kind="Rounding")
final_model = xgboost(data = xgb_train, max.depth = 3, nrounds = 51, verbose = 0)

#Make predictions on the test set and compute RMSE
predict_XG <- predict(final_model, newdata = xgb_test)
RMSE_XG <- caret::RMSE(test_y, predict_XG)

```

The table below compares the XGBoost model to the linear regression on the basis of RMSE. The XGBoost model, with an RMSE of 8.290 did not perform as well as the linear regression, with an RMSE of 7.950.

```{r table XGBoost}

#Add the XGBoost model to the table
model_results <- bind_rows(model_results, 
                           tibble(Model = "XGBoost", 
                                  Adjusted_R2 = NA,
                                  RMSE = RMSE_XG))
model_results %>% knitr::kable()
```

## 3.3 Principal Components Regression Model (PCR)

Below is the code for fitting and tuning the PCR model.

```{r PCR}

#Fit the PCR model
library(pls)
model_PCR <- pcr(UCLA_index ~., data = train, scale=TRUE, validation="CV")

#View a summary of the model fitting and select number of components that yields
#the lowest RMSE in cross-validation
summary(model_PCR)

#view plot to confirm that 33 components gives the lowest RMSE
validationplot(model_PCR)

#Use the model to make predictions on the test set
predict_PCR <- predict(model_PCR, test, ncomp=33)

#Compute RMSE for PCR model in the test set
RMSE_PCR <- RMSE(test$UCLA_index, predict_PCR)
RMSE_PCR
```

The table below shows the PCR results compared to linear regression and XGBoost. The PCR model, with an RMSE of 7.982, performed better than the XGBoost but not as well as the linear regression.

```{r table PCR}

#Add the PCR results to the table
model_results <- bind_rows(model_results, 
                           tibble(Model = "PCR", 
                                  Adjusted_R2 = NA,
                                  RMSE = RMSE_PCR))
model_results %>% knitr::kable()
```

## 3.4 Ensemble Model

Below is the code for creating an ensemble from the average of the predictions of the linear regression, XGBoost and PCR models.

```{r ensemble}

#Create a dataframe with the predictions from the LM, XGBoost and PCR models
ensemble_preds <- data.frame(LM_preds = predict_LM, 
                             XG_preds = predict_XG,
                             PCR_preds = as.vector(predict_PCR),
                             Avg = as.vector((predict_LM + predict_XG +
                                                predict_PCR)/3))

#compute RMSE for the ensemble
RMSE_ensemble <- RMSE(test$UCLA_index, ensemble_preds$Avg)
RMSE_ensemble
```

As the table below shows, the ensemble, with an RMSE of 7.91, performed better than the other three models.

```{r table ensemble}

#Add the ensemble to the table
model_results <- bind_rows(model_results, 
                           tibble(Model = "Ensemble", 
                                  Adjusted_R2 = NA,
                                  RMSE = RMSE_ensemble))
model_results %>% knitr::kable()
```

## 3.5 Variable Importance

I used the linear regression and XGBoost models to provide rankings of how important each of the variables was to predicting loneliness.

First I generated the top five predictors from the linear regression based on absolute value of the t-values.

```{r linear regression variable importance}

#Extract the absolute value of the t-values from the linear regression model
#summary and convert to a data frame
LM_tvalues <- as.data.frame(LMsummary$coefficients[-1 ,3])
LM_tvalues <- data.frame(Variable = rownames(LM_tvalues), LM_tvalues)
rownames(LM_tvalues) <- NULL
names(LM_tvalues)[2] <- "t_values"

#Take the absolute value of the t-values and sort highest to lowest
LM_tvalues$t_values <- abs(LM_tvalues$t_values)
LM_tvalues <- arrange(LM_tvalues, desc(t_values))
#Extract the top five predictors fromthe linear regression model
top_five_LM <- head(LM_tvalues, 5)
top_five_LM
```

Then I generated the top five predictors from the XGBoost model based the Gain metric in the feature importance matrix.

```{r XGBoost variable importance}

# Generate the feature importance matrix for the XGBoost model
XGimportance_matrix = xgb.importance(colnames(xgb_train), model = final_model)
XGimportance_matrix <- XGimportance_matrix[ , 1:2]

#The matrix is already sorted in descending order
#Extract the top five predictors from the XGBoost model
top_five_XG <- head(XGimportance_matrix, 5)
top_five_XG

```

Comparing the two lists, we see that they have four factors in common:

-   Hours spent physically alone (Q77_hrs_alone)

-   Relationships with neighbors (NeighborIndex)

-   Agreement with the statement that "the more I use the internet as a replacement for other forms of communication, the lonelier I feel" (Q89_5\_internet_sentiment)

-   Change in numbers of friends over the last five years. (Q38_more_less_friends)

Thus, these four factors are consistently found to be top predictors of loneliness.

# 4. Conclusion

## 4.1 Discussion of Results and Potential Impact

For this project, I created four machine learning models to predict loneliness from a set of social, technology, and demographic factors. The goals were to (a) model the relationships with better performance in terms of R-squared than the AARP model, (b) maximize accuracy in the test set on the basis of RMSE, and (c) identify the features most important to predicting loneliness.

My linear regression model substantially out-performed the AARP model. With an R-squared of 41.5%, my model explained almost double the variability in the loneliness measure as the AARP model, which had an R-squared of 21.3%. Although both of these values may appear low, it is not unusual in the social sciences to see R-squared values less than 50%. It was interesting to see that my model with 23 predictors did better than the AARP model with more than 50 predictors. Typically, adding more variables to a linear regression model increases R-squared. Perhaps greater selectivity reduced redundancy (multicollinearity) or eliminated irrelevant variables, thereby improving the model.

Of the four machine learning models I evaluated on the basis of RMSE, the ensemble performed best. Ensemble methods typically perform better than their constituent models, so this was not surprising. The next best performing model was the linear regression. I had expected XGBoost to perform better than linear regression because it would do a better job of fitting categorical variables, but in the current project it did not offer any improvement. Nor did the PCR. It was notable in the PCR model that the optimal number of components was 33, indicating that it did not offer any dimension reduction.

I examined feature importance using the linear regression and XGBoost models. The two models identified a fairly consistent set of top predictors. Four of the top five were the same for both models:

-   Hours spent physically alone

-   Relationships with neighbors

-   Agreement that using the internet as a replacement for other forms of communication leads to loneliness

-   Change in number of friends over the past five years.

Three of these predictors suggest that offline interaction is helpful for avoiding loneliness. Hours spent physically alone were positively associated with loneliness; to avoid hours spent physically alone, we need to engage with others face-to-face rather then online. The NeighborIndex variable includes questions about how often you speak to neighbors and how strong a relationship you have with them. More interaction with neighbors helps to minimize loneliness, and it is conceivable that because neighbors are physically proximate, more of these interactions occur face-to-face. While the question asking respondents whether they agree that *using the internet as a replacement for other forms of communication leads to loneliness* didn't measure their actual behavior, it nonetheless suggests that people who recognize the value of offline communications tend to be less lonely.

The current findings and their implications for offline interactions are consistent with prior research exploring how social technologies affect loneliness. According to a review by Nowland et al [@nowland2017], digital social interaction can reduce loneliness when it is used as a way to enhance existing offline relationships or forge new relationships, but when social technologies are used to displace offline relationships and avoid the messy demands of face-to-face interaction, feelings of loneliness increase. Thus, maintaining an offline social network is necessary for avoiding loneliness. A study by Twenge et al [@twenge2019], however, indicates that, at least in teen populations, offline interaction is being eroded by social technologies, with corresponding increases in loneliness. They found that compared to previous generations, today's teens are spending less face time with friends in activities such as hanging out at the mall, going to parties, dating, riding in cars for fun, or going to the movies, and these reductions correspond to escalating levels of loneliness. In 2017, 39% of 12th graders felt lonely, up from 26% in 2012, when the use of social technologies began to soar. Granted, the Twenge study was performed on teens/young adults and ours examines an older adult population. Studies such as these nonetheless raise the question of how our diminishing face-to-face time affects feelings of loneliness for people of any age.

## 4.2 Limitations

-   The AARP data were collected from a sample of respondents age 45 and above. We therefore need to be cautious about applying the findings to younger populations.

-   The AARP survey was not hypothesis driven and because I didn't design the survey questions, I was unable to directly test hypotheses (e.g., about the relationship between social technology use and loneliness.)

-   The AARP survey included ten questions about "internet sentiment", asking how strongly respondents agreed with a set of statements such, "The more I use the internet as a replacement for other forms of communication, the lonelier I feel." The survey did not ask about actual behaviors, e.g., whether respondents have in fact used the internet as a replacement for other forms of communication. We are therefore limited in our ability to draw conclusions about the relationships between *behaviors* and loneliness from these ten questions--only about *attitudes* and loneliness.

## 4.3 Future Work

Additional hypothesis-driven studies should be conducted to better understand the relationships between online and offline interactions and loneliness. Further, because loneliness is most common among teens and young adults (ages \~16 - 24), future work should examine similar factors in this population. The existing literature does not often explore relationships with neighbors as a factor in reducing loneliness. Further research should be conducted to better understand the role and importance of neighborhood relationships.

# Appendix A: AARP Variables

![](Appendix_A_AARP_variables.png)

# References
